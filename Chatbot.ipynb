{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "etmFQ8Um9nik"
   },
   "source": [
    "# Importing The Neccesary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "sJj-U60xSq2C"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import json\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k-9rfsFB9xZV"
   },
   "source": [
    "# Creating Query-Based Intents for the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "xaHOpKhSSsRR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before appending 273\n",
      "after appending 274\n",
      "after appending 275\n",
      "after appending 276\n",
      "after appending 277\n",
      "after appending 278\n",
      "after appending 279\n",
      "after appending 280\n",
      "after appending 281\n",
      "after appending 282\n",
      "after appending 283\n",
      "after appending 284\n",
      "after appending 285\n",
      "after appending 286\n",
      "after appending 287\n"
     ]
    }
   ],
   "source": [
    "with open(\"C:/Users/HP/Downloads/intents_file.txt\",\"r\") as file:\n",
    "    file_content = file.read()\n",
    "    intents = dict(eval(file_content))\n",
    "    print(\"before appending\",len(intents[\"intents\"]))\n",
    "\n",
    "conn = sqlite3.connect(r\"C:/Users/HP/Downloads/chatbot_db.db\")\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(\"SELECT tag, patterns, responses FROM student_response\")\n",
    "records = cursor.fetchall()\n",
    "if 50 >= 50:\n",
    "        for record in records:\n",
    "            tag = record[0]\n",
    "            patterns = [record[1]]\n",
    "            responses = [record[2]]\n",
    "            intent = {\n",
    "                \"tag\": tag,\n",
    "                \"patterns\": patterns,\n",
    "                \"responses\": responses\n",
    "            }\n",
    "            intents[\"intents\"].append(intent)\n",
    "            with open(\"C:/Users/HP/Downloads/intents_file.txt\",\"w\") as file:\n",
    "                file.write(str(intents))\n",
    "            print(\"after appending\",len(intents[\"intents\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ky1-vGtU-TDq"
   },
   "source": [
    "### Downloading NLP Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d713ZY5OTGMB",
    "outputId": "410716dc-d81f-4d5f-fde6-300d6ea650d8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hIHwh_cL-gYh"
   },
   "source": [
    "# Processing Data and Synonym Augmentation for Intent Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0Sy6LDcr-2SA",
    "outputId": "30b9e726-d662-4332-c431-4a8fbd8bbafd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1096 documents\n",
      "223 classes ['About Canteen', 'Greeting of Hi', 'Greetings of Hi', 'academic_advice_services', 'academic_advising', 'academic_advising_system', 'academic_advisory_system', 'academic_credit_transfer', 'academic_credits_transfer', 'academic_system', 'academics_majors', 'academics_reputation', 'accessibility', 'admission', 'admission_requirements_international_students', 'admissions_interview_role', 'alumni_association', 'alumni_network', 'alumni_network_activity', 'athletic_scholarships', 'average_class_size_intro', 'average_class_size_upper_division', 'average_federal_plus_loan_debt', 'average_financial_aid_package', 'average_loan_debt', 'average_merit_award', 'bond_rating', 'campus_diversity', 'campus_facilities', 'campus_housing_percentage', 'campus_navigation', 'campus_safety_security', 'campus_wifi', 'canteen_items', 'career_planning_services', 'career_services', 'career_services_access', 'career_services_alumni', 'career_services_for_alumni', 'casual_greeting', 'class_format', 'class_size', 'class_sizes', 'classes_professors_tas', 'co-op_program', 'co_op_program', 'collaborative_work', 'college_changes_suggestion', 'college_choice_reasons', 'college_likes_least', 'college_likes_most', 'college_pride', 'communication', 'complaints', 'computer_labs', 'contact', 'course_enrollment_ease', 'courses', 'creator', 'declaring_major', 'demonstrated_interest_admissions', 'departments_offer_undergrad_research', 'departments_with_research', 'disability_testing_requirements', 'dorm_quality', 'dormitories_after_freshman_year', 'double_major', 'double_major_graduation_time', 'double_major_impact_graduation', 'drug_alcohol_prevalence', 'employer_recruitment', 'enrollment_capped_majors', 'entrepreneurship_resources_clubs', 'events', 'experiential_learning_internships', 'extra_scholarships_majors_talents', 'facilities', 'farewell', 'favorite_class', 'favorite_place_on_campus', 'feeder_institutions_graduate_schools', 'fees', 'fellow_students', 'financial_aid_after_four_years', 'financial_aid_grants_scholarships', 'financial_aid_international_students', 'financial_aid_loans_vs_grants', 'financial_aid_percentage', 'financial_aid_renewable', 'financial_literacy_training', 'financial_need_met_freshman_year', 'financial_need_met_percentage', 'financial_stability', 'first_choice_classes', 'first_year_experience', 'five_year_graduation_rate', 'food', 'four_year_graduation_rate', 'fraternities_sororities', 'freshman_retention_rate', 'freshmen_bring_car', 'freshmen_mentoring_program', 'freshmen_retention_rate', 'friendly_students', 'good_afternoon', 'good_evening', 'good_morning', 'grad_school_application_success', 'graduate_school_admission_percentage', 'graduate_students', 'graduation_in_four_years', 'graduation_on_time', 'graduation_rate_five_years', 'graduation_rate_four_years', 'graduation_rate_six_years', 'graduation_reasons_delay', 'greeting', 'guaranteed_major', 'hands-on_experiences', 'happiness', 'hometown_activities', 'honors_college', 'honors_college_offering', 'honors_college_requirements', 'hostel', 'hours', 'housing_choices', 'identity', 'impacted_majors', 'improvements', 'independent_study', 'interaction_with_professors', 'interactions_among_students', 'international_students', 'internship_opportunities', 'internship_percentage', 'job_placement_rate', 'job_placement_rate_calculation', 'leadership_opportunities', 'learning_community', 'learning_disability_services', 'legacy_admissions', 'library', 'location', 'major_admission_standards', 'major_requirements_breakdown', 'meal_plan_options', 'mental_health_services', 'mentorship_program', 'merit_scholarship_requirements', 'merit_scholarships_info', 'net_price_calculator_accuracy', 'net_price_calculator_update', 'non_class_activities', 'notable_graduates', 'on_campus_housing', 'orientation_program', 'participating_students_undergrad_research', 'percentage_students_studies_abroad', 'percentage_students_study_abroad', 'performing_arts_music_facilities', 'placement', 'popular_clubs', 'professor_accessibility', 'professor_as_advisor', 'professor_research', 'professors_office_hours', 'professors_teachers', 'professors_vs_TAs', 'reading_writing', 'reasons_for_premature_leaving', 'reasons_students_leave', 'recommended_students', 'recreational_facilities', 'requirements_to_graduate_4years', 'roommate_assignment', 'school_choice', 'school_spirit', 'self_reported_job_placement_rate', 'semester_abroad', 'senior_capstone_experience', 'service_learning', 'six_year_graduation_rate', 'social_scene', 'sorority_fraternity_percentage', 'sorry', 'special_features', 'sports_popularity', 'strengths', 'student_body_unique', 'student_cliques', 'student_loan_default_rate', 'student_organizations', 'student_to_faculty_ratio', 'students_not_fit_in', 'study_abroad', 'study_abroad_intended_major', 'study_abroad_percentage', 'study_abroad_programs', 'study_locations', 'study_time', 'successful_students', 'summer_jobs', 'sustainability_initiatives', 'switch_majors_within_university', 'teaching_assistants', 'test_optional_policy_exceptions', 'thank_you', 'theoretical_or_hands_on', 'time_spent_on_homework', 'transportation_options', 'tuition_rate_increase', 'tutoring_programs', 'tutoring_services', 'typical_class_sizes_major', 'typical_day', 'undergraduate_research_opportunities', 'undergraduate_research_participation', 'undergraduate_students', 'weekend_activities', 'work_study_opportunities', 'writing_and_reading_expectations', 'writing_center']\n",
      "987 unique lemmatized words ['#', \"'\", \"'d\", \"'m\", \"'re\", \"'s\", \"'ve\", '(', ')', ',', '.', '1', ':', 'a', 'able', 'about', 'abroad', 'academic', 'accepted', 'access', 'accessible', 'accommodation', 'accomplishment', 'account', 'accumulate', 'accurate', 'achievement', 'across', 'action', 'active', 'activity', 'adapt', 'additional', 'address', 'adjusting', 'administrator', 'admission', 'advanced', 'advancement', 'advantage', 'advice', 'advising', 'advisor', 'advisory', 'affect', 'after', 'afternoon', 'aid', 'alcohol', 'all', 'alternative', 'alumnus', 'am', 'ambition', 'among', 'amount', 'an', 'and', 'and/or', 'annual', 'another', 'anxiety', 'any', 'anything', 'apart', 'apology', 'applicable', 'applicant', 'application', 'apply', 'applying', 'appointment', 'approach', 'are', 'area', 'around', 'art', 'aspect', 'aspiring', 'assigned', 'assigning', 'assignment', 'assist', 'assistance', 'assistant', 'association', 'at', 'athletic', 'athletics', 'attend', 'attended', 'attending', 'attention', 'attracted', 'attraction', 'attrition', 'audited', 'automatically', 'availability', 'available', 'average', 'award', 'awarded', 'background', 'based', 'be', 'become', 'being', 'belong', 'belonging', 'benefit', 'best', 'better', 'between', 'beyond', 'big', 'bike-friendly', 'bike-sharing', 'body', 'bond', 'borrowed', 'branch', 'breakdown', 'bring', 'building', 'but', 'by', 'bye', 'calculated', 'calculator', 'calendar', 'call', 'campus', 'can', 'canteen', 'capacity', 'capped', 'capstone', 'car', 'career', 'center', 'centralized', 'certain', 'challenge', 'chance', 'change', 'channel', 'characteristic', 'chatbot', 'child', 'choice', 'choose', 'choosing', 'circumstance', 'citizen', 'clarification', 'class', 'classroom', 'clique', 'cliquish', 'close', 'club', 'co-op', 'collaborate', 'collaborative', 'collect', 'college', 'comfortable', 'commitment', 'committed', 'common', 'commonly', 'communicate', 'communication', 'community', 'company', 'compatibility', 'competitive', 'complaint', 'complete', 'completion', 'complex', 'component', 'comprised', 'computer', 'concern', 'conduct', 'connect', 'connected', 'connectivity', 'consider', 'considered', 'considering', 'consist', 'contact', 'continue', 'contribute', 'cooperative', 'core', 'cost', 'could', 'counseling', 'count', 'counted', 'country', 'course', 'coursework', 'created', 'creative', 'credit', 'criterion', 'cross-cultural', 'culminating', 'cultural', 'culture', 'current', 'curriculum', 'daily', 'data', 'day', 'deadline', 'debt', 'decide', 'decision', 'decision-making', 'declare', 'declaring', 'decrease', 'dedicate', 'default', 'degree', 'delay', 'demographic', 'demonstrated', 'demonstrating', 'department', 'departure', 'describe', 'designated', 'designed', 'desired', 'determine', 'develop', 'developer', 'development', 'did', 'dietary', 'difference', 'different', 'difficult', 'dining', 'direct', 'disability', 'disappointing', 'discipline', 'discover', 'discrepancy', 'discussion', 'discussion-based', 'distinctive', 'diverse', 'diversity', 'diversity-related', 'do', 'document', 'documentation', 'doe', 'doing', 'dorm', 'dormitory', 'double', 'drawback', 'drug', 'during', 'each', 'earn', 'easier', 'easily', 'easy', 'eco-friendly', 'educate', 'education', 'effective', 'effort', 'elective', 'eligibility', 'eligible', 'else', 'email', 'embrace', 'emphasis', 'emphasize', 'employer', 'employment', 'encounter', 'encourage', 'encouraged', 'endowment', 'engage', 'engaged', 'english', 'enhance', 'enhanced', 'enjoy', 'enough', 'enroll', 'enrollment', 'ensure', 'entrepreneur', 'entrepreneurial', 'entrepreneurship', 'environmental', 'essay', 'estimate', 'etc', 'evaluated', 'evaluation', 'evening', 'event', 'example', 'exception', 'exceptional', 'exchange', 'exclusive', 'exist', 'expect', 'expected', 'experience', 'experienced', 'experiencing', 'experiential', 'exploration', 'externally', 'extra', 'extracurricular', 'face', 'facilitate', 'facility', 'factor', 'faculty', 'fair', 'family', 'famous', 'favorite', 'feature', 'federal', 'fee', 'feeder', 'feel', 'fellow', 'field', 'figure', 'finance', 'financial', 'financially', 'find', 'finding', 'first', 'first-choice', 'first-semester', 'first-year', 'fit', 'fitness', 'five', 'five-year', 'flexibility', 'focus', 'food', 'for', 'form', 'format', 'four', 'four-year', 'frame', 'fraternity', 'free', 'freeze', 'frequently', 'freshman', 'friend', 'friendly', 'from', 'frustrating', 'full', 'fund', 'gain', 'gap', 'general', 'generally', 'get', 'getting', 'give', 'given', 'go', 'goal', 'good', 'goodbye', 'gpa', 'grad', 'grade', 'graduate', 'graduated', 'graduating', 'graduation', 'grant', 'greek', 'green', 'group', 'guarantee', 'guaranteed', 'guidance', 'guideline', 'gym', 'ha', 'had', 'hall', 'handle', 'hands-on', 'hang', 'happens', 'happiest', 'happy', 'have', 'having', 'health', 'heavy', 'hello', 'help', 'helpful', 'here', 'hey', 'hi', 'high', 'high-achieving', 'higher', 'highest', 'highlight', 'hold', 'hometown', 'homework', 'honor', 'hostel', 'hour', 'hourly', 'housing', 'how', 'how/when', 'i', 'if', 'immediately', 'impacted', 'impacted/oversubscribed', 'important', 'improve', 'improved', 'improvement', 'in', 'include', 'inclusion', 'inclusivity', 'incoming', 'increase', 'incur', 'independent', 'individual', 'influenced', 'information', 'informed', 'initiative', 'institution', 'intended', 'interact', 'interaction', 'interest', 'interested', 'international', 'internship', 'interview', 'intimate', 'into', 'intramural', 'introduce', 'introductory', 'involve', 'involved', 'involvement', 'involving', 'is', 'issue', 'it', 'iâ€™m', 'job', 'join', 'joint', 'journey', 'junior', 'keep', 'kind', 'know', 'knowledge', 'known', 'lab', 'lacking', 'language', 'large', 'last', 'later', 'layout', 'le', 'lead', 'leadership', 'learn', 'learning', 'least', 'leave', 'leaving', 'lecture', 'lecture-based', 'legacy', 'length', 'level', 'library', 'life', 'like', 'likely', 'limit', 'limitation', 'limited', 'list', 'literacy', 'live', 'living', 'loan', 'local', 'located', 'location', 'longer', 'lose', 'lot', 'made', 'main', 'maintain', 'major', 'major-related', 'major-specific', 'majoring', 'make', 'manage', 'manageable', 'managing', 'mandatory', 'many', 'match', 'matching', 'maximum', 'may', 'me', 'meal', 'measure', 'meet', 'meeting', 'member', 'mental', 'mention', 'mentor', 'mentoring', 'mentorship', 'merit', 'merit-based', 'might', 'mode', 'modern', 'more', 'morning', 'most', 'much', 'multiple', 'music', 'musician', 'must', 'my', \"n't\", 'name', 'nature', 'navigate', 'near', 'need', 'need-based', 'needed', 'net', 'network', 'networking', 'new', 'non-us', 'not', 'notable', 'number', 'obstacle', 'of', 'offer', 'offered', 'office', 'officer', 'often', 'on', 'on-campus', 'on-time', 'one', 'one-on-one', 'open', 'operation', 'opportunity', 'option', 'or', 'organization', 'organized', 'orientation', 'other', 'others', 'out', 'outcome', 'outside', 'over', 'overloading', 'own', 'pace', 'package', 'paper', 'parent', 'parking', 'part', 'part-time', 'participate', 'participating', 'participation', 'partnership', 'past', 'path', 'peer', 'people', 'per', 'percentage', 'performance', 'performing', 'place', 'placement', 'plan', 'planned', 'planning', 'play', 'plus', 'point', 'policy', 'popular', 'population', 'position', 'possibility', 'possible', 'potential', 'practical', 'practice', 'preference', 'preferred', 'prematurely', 'preparation', 'prepare', 'prerequisite', 'presence', 'presentation', 'prevalent', 'prevent', 'previous', 'price', 'pride', 'primarily', 'prioritize', 'priority', 'process', 'professional', 'professor', 'proficiency', 'program', 'project', 'promote', 'proportion', 'prospective', 'proud', 'provide', 'provided', 'public', 'purpose', 'pursue', 'pursuing', 'qualify', 'quality', 'quarter', 'question', 'quiet', 'quit', 'race', 'rate', 'rating', 'ratio', 'reach', 'reading', 'real-world', 'reason', 'receive', 'received', 'receiving', 'recent', 'recommend', 'recommended', 'record', 'recreational', 'recruit', 'registrar', 'registration', 'regular', 'regularly', 'related', 'reliable', 'rely', 'renew', 'renewable', 'renewal', 'renewed', 'repayment', 'representation', 'reputation', 'request', 'require', 'required', 'requirement', 'research', 'resource', 'respond', 'responsiveness', 'restricted', 'restriction', 'result', 'resume', 'retention', 'return', 'reviewed', 'risen', 'role', 'room', 'roommate', 'routine', 'rule', 'safe', 'safety', 'same', 'satisfied', 'scene', 'schedule', 'scholarship', 'school', 'science', 'score', 'search', 'second', 'secure', 'security', 'see', 'seek', 'segregate', 'selection', 'self-reported', 'selling', 'semester', 'senior', 'sense', 'separate', 'sequence', 'service', 'session', 'set', 'shared', 'shortage', 'should', 'show', 'showcase', 'shrink', 'significant', 'significantly', 'simultaneously', 'single', 'six', 'six-year', 'size', 'skill', 'small', 'small-sized', 'smooth', 'so', 'social', 'socialization', 'some', 'sophomore', 'sorority', 'sorry', 'sought-after', 'sound', 'space', 'special', 'specialized', 'specific', 'spend', 'spirit', 'spirited', 'sport', 'spot', 'stable', 'staff', 'stand', 'standard', 'standardized', 'standing', 'standout', 'start', 'status', 'stay', 'steady', 'step', 'still', 'straightforward', 'strategy', 'strength', 'stress', 'strong', 'structured', 'struggling', 'student', 'student-athletes', 'student-friendly', 'student-run', 'student-to-faculty', 'studentâ€™s', 'study', 'studying', 'style', 'subject', 'submit', 'submitting', 'subsequent', 'substance', 'substance-free', 'succeed', 'success', 'successful', 'successfully', 'such', 'summer', 'support', 'sure', 'surrounding', 'survey', 'sustainability', 'switch', 'switching', 'symposium', 'system', 'ta', 'take', 'taken', 'talent', 'talk', 'taught', 'teach', 'teacher', 'teaching', 'teamwork', 'teenager', 'telephone', 'tell', 'temporary', 'tend', 'tenured', 'test', 'test-optional', 'testing', 'than', 'thank', 'thanks', 'that', 'the', 'theater', 'their', 'them', 'theoretical', 'there', 'they', 'thing', 'think', 'this', 'thrive', 'thrives', 'through', 'throughout', 'time', 'timeframe', 'timeline', 'timely', 'timing', 'to', 'total', 'towards', 'track', 'tradition', 'training', 'transfer', 'transferred', 'transferring', 'transition', 'transportation', 'trimester', 'tuition', 'tutoring', 'two', 'type', 'typical', 'typically', 'undergrad', 'undergraduate', 'unique', 'university', 'up', 'upcoming', 'update', 'updated', 'upper-division', 'upper-level', 'upperclassmen', 'usage', 'use', 'used', 'user', 'using', 'usually', 'variety', 'vary', 'venue', 'verified', 'verify', 'versus', 'visible', 'visited', 'voluntarily', 'wa', 'wage', 'waitlists', 'want', 'week', 'weekend', 'well', 'well-maintained', 'well-regarded', 'were', 'what', 'when', 'where', 'which', 'while', 'who', 'why', 'wifi', 'will', 'wish', 'with', 'within', 'without', 'work', 'work-study', 'workload', 'workshop', 'would', 'writing', 'writing-intensive', 'written', 'year', 'you', 'your', 'yourself', 'â€', '“']\n"
     ]
    }
   ],
   "source": [
    "stemmer = WordNetLemmatizer()\n",
    "\n",
    "words = []\n",
    "classes = []\n",
    "documents = []\n",
    "ignore_words = ['?']\n",
    "\n",
    "\n",
    "for intent in intents['intents']:\n",
    "    for pattern in intent['patterns']:\n",
    "        # Tokenize each word in the sentence\n",
    "        w = nltk.word_tokenize(pattern)\n",
    "        # Add to our words list\n",
    "        words.extend(w)\n",
    "        # Add to documents in our corpus\n",
    "        documents.append((w, intent['tag']))\n",
    "        # Add to our classes list\n",
    "        if intent['tag'] not in classes:\n",
    "            classes.append(intent['tag'])\n",
    "\n",
    "\n",
    "words = [stemmer.lemmatize(w.lower()) for w in words if w not in ignore_words]\n",
    "words = sorted(list(set(words)))\n",
    "\n",
    "\n",
    "classes = sorted(list(set(classes)))\n",
    "\n",
    "print(len(documents), \"documents\")\n",
    "print(len(classes), \"classes\", classes)\n",
    "print(len(words), \"unique lemmatized words\", words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "bVhFUZKbZkHa"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training = []\n",
    "output = []\n",
    "output_empty = [0] * len(classes)\n",
    "\n",
    "# Training set, bag of words for each sentence\n",
    "for doc in documents:\n",
    "    # Initialize our bag of words\n",
    "    bag = []\n",
    "    # List of tokenized words for the pattern\n",
    "    pattern_words = doc[0]\n",
    "    # Lemmatize each word\n",
    "    pattern_words = [stemmer.lemmatize(word.lower()) for word in pattern_words]\n",
    "    # Create our bag of words array\n",
    "    for w in words:\n",
    "        bag.append(1) if w in pattern_words else bag.append(0)\n",
    "\n",
    "    # Output is a '0' for each tag and '1' for the current tag\n",
    "    output_row = list(output_empty)\n",
    "    output_row[classes.index(doc[1])] = 1\n",
    "\n",
    "    training.append([bag, output_row])\n",
    "\n",
    "random.shuffle(training)\n",
    "\n",
    "\n",
    "def synonym_replacement(tokens, limit):\n",
    "    augmented_sentences = []\n",
    "    for i in range(len(tokens)):\n",
    "        synonyms = []\n",
    "        for syn in wordnet.synsets(tokens[i]):\n",
    "            for lemma in syn.lemmas():\n",
    "                synonyms.append(lemma.name())\n",
    "        if len(synonyms) > 0:\n",
    "            num_augmentations = min(limit, len(synonyms))\n",
    "            sampled_synonyms = random.sample(synonyms, num_augmentations)\n",
    "            for synonym in sampled_synonyms:\n",
    "                augmented_tokens = tokens[:i] + [synonym] + tokens[i + 1:]\n",
    "                augmented_sentences.append(' '.join(augmented_tokens))\n",
    "    return augmented_sentences\n",
    "\n",
    "\n",
    "# Augment the training data using synonym replacement\n",
    "augmented_data = []\n",
    "limit_per_tag = 100\n",
    "\n",
    "for i, doc in enumerate(training):\n",
    "    bag, output_row = doc\n",
    "    tokens = [words[j] for j in range(len(words)) if bag[j] == 1]\n",
    "    augmented_sentences = synonym_replacement(tokens, limit_per_tag)\n",
    "    for augmented_sentence in augmented_sentences:\n",
    "        augmented_bag = [1 if augmented_sentence.find(word) >= 0 else 0 for word in words]\n",
    "        augmented_data.append([augmented_bag, output_row])\n",
    "\n",
    "\n",
    "\n",
    "print(\"training: \\n\",training)\n",
    "print(\"augmented: \\n\",augmented_data)\n",
    "\n",
    "training_array = np.array([(doc[0], doc[1]) for doc in training], dtype=object)\n",
    "augmented_array = np.array([(doc[0], doc[1]) for doc in augmented_data], dtype=object)\n",
    "\n",
    "combined_data = np.concatenate((training_array, augmented_array), axis=0)\n",
    "np.random.shuffle(combined_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5tqBNJTMdPr7"
   },
   "source": [
    "# Splitting the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "7plFRFXp5Zb9"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def separate_data_by_tags(data):\n",
    "    data_by_tags = {}\n",
    "    for d in data:\n",
    "        tag = tuple(d[1])\n",
    "        if tag not in data_by_tags:\n",
    "            data_by_tags[tag] = []\n",
    "        data_by_tags[tag].append(d)\n",
    "    return data_by_tags.values()\n",
    "\n",
    "\n",
    "separated_data = separate_data_by_tags(combined_data)\n",
    "\n",
    "# Lists to store training and testing data\n",
    "training_data = []\n",
    "testing_data = []\n",
    "\n",
    "# Split each tag's data into training and testing sets\n",
    "for tag_data in separated_data:\n",
    "    train_data, test_data = train_test_split(tag_data, test_size=0.2, random_state=42)\n",
    "    training_data.extend(train_data)\n",
    "    testing_data.extend(test_data)\n",
    "\n",
    "\n",
    "random.shuffle(training_data)\n",
    "random.shuffle(testing_data)\n",
    "\n",
    "# Convert training and testing data back to np.array\n",
    "train_x = np.array([d[0] for d in training_data])\n",
    "train_y = np.array([d[1] for d in training_data])\n",
    "test_x = np.array([d[0] for d in testing_data])\n",
    "test_y = np.array([d[1] for d in testing_data])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LE7ekzsmdbui"
   },
   "source": [
    "# Training The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "88P0X4kJ6R5T"
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_size)\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_size)\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "\n",
    "        self.fc3 = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        x = self.fc3(x)\n",
    "        output = self.softmax(x)\n",
    "        return output\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return self.softmax(x)\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]\n",
    "\n",
    "def accuracy(predictions, targets):\n",
    "    predicted_labels = torch.argmax(predictions, dim=1)\n",
    "    true_labels = torch.argmax(targets, dim=1)\n",
    "    correct = (predicted_labels == true_labels).sum().item()\n",
    "    total = targets.size(0)\n",
    "    return correct / total\n",
    "\n",
    "def test_model(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_accuracy = 0.0\n",
    "    num_batches = len(test_loader)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            total_loss += loss.item() * inputs.size(0)\n",
    "            total_accuracy += accuracy(outputs, targets) * inputs.size(0)\n",
    "\n",
    "    average_loss = total_loss / len(test_loader.dataset)\n",
    "    average_accuracy = total_accuracy / len(test_loader.dataset)\n",
    "    return average_loss, average_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pw_J8A3l6c26",
    "outputId": "69d69f34-a40f-47bc-a558-e3ac2cd39036"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Training Loss: 0.0201, Training Accuracy: 0.2357\n",
      "Epoch [1/50], Testing Loss: 0.0130, Testing Accuracy: 0.4812\n",
      "Epoch [2/50], Training Loss: 0.0103, Training Accuracy: 0.6013\n",
      "Epoch [2/50], Testing Loss: 0.0083, Testing Accuracy: 0.6817\n",
      "Epoch [3/50], Training Loss: 0.0072, Training Accuracy: 0.7322\n",
      "Epoch [3/50], Testing Loss: 0.0061, Testing Accuracy: 0.7784\n",
      "Epoch [4/50], Training Loss: 0.0052, Training Accuracy: 0.8172\n",
      "Epoch [4/50], Testing Loss: 0.0044, Testing Accuracy: 0.8491\n",
      "Epoch [5/50], Training Loss: 0.0039, Training Accuracy: 0.8638\n",
      "Epoch [5/50], Testing Loss: 0.0035, Testing Accuracy: 0.8804\n",
      "Epoch [6/50], Training Loss: 0.0032, Training Accuracy: 0.8876\n",
      "Epoch [6/50], Testing Loss: 0.0030, Testing Accuracy: 0.8901\n",
      "Epoch [7/50], Training Loss: 0.0028, Training Accuracy: 0.9017\n",
      "Epoch [7/50], Testing Loss: 0.0027, Testing Accuracy: 0.9055\n",
      "Epoch [8/50], Training Loss: 0.0025, Training Accuracy: 0.9118\n",
      "Epoch [8/50], Testing Loss: 0.0024, Testing Accuracy: 0.9109\n",
      "Epoch [9/50], Training Loss: 0.0022, Training Accuracy: 0.9193\n",
      "Epoch [9/50], Testing Loss: 0.0022, Testing Accuracy: 0.9207\n",
      "Epoch [10/50], Training Loss: 0.0020, Training Accuracy: 0.9252\n",
      "Epoch [10/50], Testing Loss: 0.0020, Testing Accuracy: 0.9227\n",
      "Epoch [11/50], Training Loss: 0.0019, Training Accuracy: 0.9295\n",
      "Epoch [11/50], Testing Loss: 0.0019, Testing Accuracy: 0.9248\n",
      "Epoch [12/50], Training Loss: 0.0018, Training Accuracy: 0.9327\n",
      "Epoch [12/50], Testing Loss: 0.0018, Testing Accuracy: 0.9324\n",
      "Epoch [13/50], Training Loss: 0.0017, Training Accuracy: 0.9365\n",
      "Epoch [13/50], Testing Loss: 0.0018, Testing Accuracy: 0.9335\n",
      "Epoch [14/50], Training Loss: 0.0016, Training Accuracy: 0.9381\n",
      "Epoch [14/50], Testing Loss: 0.0017, Testing Accuracy: 0.9320\n",
      "Epoch [15/50], Training Loss: 0.0015, Training Accuracy: 0.9406\n",
      "Epoch [15/50], Testing Loss: 0.0017, Testing Accuracy: 0.9379\n",
      "Epoch [16/50], Training Loss: 0.0015, Training Accuracy: 0.9427\n",
      "Epoch [16/50], Testing Loss: 0.0016, Testing Accuracy: 0.9388\n",
      "Epoch [17/50], Training Loss: 0.0014, Training Accuracy: 0.9443\n",
      "Epoch [17/50], Testing Loss: 0.0016, Testing Accuracy: 0.9386\n",
      "Epoch [18/50], Training Loss: 0.0014, Training Accuracy: 0.9459\n",
      "Epoch [18/50], Testing Loss: 0.0015, Testing Accuracy: 0.9402\n",
      "Epoch [19/50], Training Loss: 0.0013, Training Accuracy: 0.9470\n",
      "Epoch [19/50], Testing Loss: 0.0015, Testing Accuracy: 0.9404\n",
      "Epoch [20/50], Training Loss: 0.0013, Training Accuracy: 0.9478\n",
      "Epoch [20/50], Testing Loss: 0.0015, Testing Accuracy: 0.9419\n",
      "Epoch [21/50], Training Loss: 0.0013, Training Accuracy: 0.9489\n",
      "Epoch [21/50], Testing Loss: 0.0014, Testing Accuracy: 0.9427\n",
      "Epoch [22/50], Training Loss: 0.0012, Training Accuracy: 0.9497\n",
      "Epoch [22/50], Testing Loss: 0.0015, Testing Accuracy: 0.9436\n",
      "Epoch [23/50], Training Loss: 0.0012, Training Accuracy: 0.9510\n",
      "Epoch [23/50], Testing Loss: 0.0014, Testing Accuracy: 0.9454\n",
      "Epoch [24/50], Training Loss: 0.0012, Training Accuracy: 0.9516\n",
      "Epoch [24/50], Testing Loss: 0.0014, Testing Accuracy: 0.9453\n",
      "Epoch [25/50], Training Loss: 0.0012, Training Accuracy: 0.9528\n",
      "Epoch [25/50], Testing Loss: 0.0014, Testing Accuracy: 0.9471\n",
      "Epoch [26/50], Training Loss: 0.0011, Training Accuracy: 0.9527\n",
      "Epoch [26/50], Testing Loss: 0.0014, Testing Accuracy: 0.9426\n",
      "Epoch [27/50], Training Loss: 0.0011, Training Accuracy: 0.9527\n",
      "Epoch [27/50], Testing Loss: 0.0014, Testing Accuracy: 0.9468\n",
      "Epoch [28/50], Training Loss: 0.0011, Training Accuracy: 0.9541\n",
      "Epoch [28/50], Testing Loss: 0.0013, Testing Accuracy: 0.9459\n",
      "Epoch [29/50], Training Loss: 0.0011, Training Accuracy: 0.9542\n",
      "Epoch [29/50], Testing Loss: 0.0013, Testing Accuracy: 0.9482\n",
      "Epoch [30/50], Training Loss: 0.0011, Training Accuracy: 0.9551\n",
      "Epoch [30/50], Testing Loss: 0.0013, Testing Accuracy: 0.9490\n",
      "Epoch [31/50], Training Loss: 0.0010, Training Accuracy: 0.9557\n",
      "Epoch [31/50], Testing Loss: 0.0013, Testing Accuracy: 0.9487\n",
      "Epoch [32/50], Training Loss: 0.0010, Training Accuracy: 0.9557\n",
      "Epoch [32/50], Testing Loss: 0.0013, Testing Accuracy: 0.9464\n",
      "Epoch [33/50], Training Loss: 0.0010, Training Accuracy: 0.9561\n",
      "Epoch [33/50], Testing Loss: 0.0013, Testing Accuracy: 0.9508\n",
      "Epoch [34/50], Training Loss: 0.0010, Training Accuracy: 0.9564\n",
      "Epoch [34/50], Testing Loss: 0.0013, Testing Accuracy: 0.9500\n",
      "Epoch [35/50], Training Loss: 0.0010, Training Accuracy: 0.9570\n",
      "Epoch [35/50], Testing Loss: 0.0013, Testing Accuracy: 0.9467\n",
      "Epoch [36/50], Training Loss: 0.0010, Training Accuracy: 0.9579\n",
      "Epoch [36/50], Testing Loss: 0.0012, Testing Accuracy: 0.9502\n",
      "Epoch [37/50], Training Loss: 0.0010, Training Accuracy: 0.9578\n",
      "Epoch [37/50], Testing Loss: 0.0012, Testing Accuracy: 0.9483\n",
      "Epoch [38/50], Training Loss: 0.0010, Training Accuracy: 0.9578\n",
      "Epoch [38/50], Testing Loss: 0.0012, Testing Accuracy: 0.9493\n",
      "Epoch [39/50], Training Loss: 0.0009, Training Accuracy: 0.9581\n",
      "Epoch [39/50], Testing Loss: 0.0013, Testing Accuracy: 0.9484\n",
      "Epoch [40/50], Training Loss: 0.0009, Training Accuracy: 0.9585\n",
      "Epoch [40/50], Testing Loss: 0.0012, Testing Accuracy: 0.9495\n",
      "Epoch [41/50], Training Loss: 0.0009, Training Accuracy: 0.9589\n",
      "Epoch [41/50], Testing Loss: 0.0012, Testing Accuracy: 0.9507\n",
      "Epoch [42/50], Training Loss: 0.0009, Training Accuracy: 0.9587\n",
      "Epoch [42/50], Testing Loss: 0.0012, Testing Accuracy: 0.9519\n",
      "Epoch [43/50], Training Loss: 0.0009, Training Accuracy: 0.9596\n",
      "Epoch [43/50], Testing Loss: 0.0012, Testing Accuracy: 0.9524\n",
      "Epoch [44/50], Training Loss: 0.0009, Training Accuracy: 0.9595\n",
      "Epoch [44/50], Testing Loss: 0.0012, Testing Accuracy: 0.9510\n",
      "Epoch [45/50], Training Loss: 0.0009, Training Accuracy: 0.9594\n",
      "Epoch [45/50], Testing Loss: 0.0012, Testing Accuracy: 0.9522\n",
      "Epoch [46/50], Training Loss: 0.0009, Training Accuracy: 0.9600\n",
      "Epoch [46/50], Testing Loss: 0.0012, Testing Accuracy: 0.9512\n",
      "Epoch [47/50], Training Loss: 0.0009, Training Accuracy: 0.9600\n",
      "Epoch [47/50], Testing Loss: 0.0013, Testing Accuracy: 0.9478\n",
      "Epoch [48/50], Training Loss: 0.0009, Training Accuracy: 0.9602\n",
      "Epoch [48/50], Testing Loss: 0.0012, Testing Accuracy: 0.9518\n",
      "Epoch [49/50], Training Loss: 0.0009, Training Accuracy: 0.9603\n",
      "Epoch [49/50], Testing Loss: 0.0012, Testing Accuracy: 0.9520\n",
      "Epoch [50/50], Training Loss: 0.0009, Training Accuracy: 0.9610\n",
      "Epoch [50/50], Testing Loss: 0.0012, Testing Accuracy: 0.9504\n"
     ]
    }
   ],
   "source": [
    "# Create DataLoader for training and testing data\n",
    "train_x = torch.tensor(train_x).float()\n",
    "train_y = torch.tensor(train_y).float()\n",
    "test_x = torch.tensor(test_x).float()\n",
    "test_y = torch.tensor(test_y).float()\n",
    "\n",
    "batch_size = 64\n",
    "train_dataset = CustomDataset(train_x, train_y)\n",
    "test_dataset = CustomDataset(test_x, test_y)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Define the model, loss function, and optimizer\n",
    "input_size = len(train_x[0])\n",
    "hidden_size = 8\n",
    "output_size = len(train_y[0])\n",
    "model = NeuralNetwork(input_size, hidden_size, output_size)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "# Train the model and evaluate on the testing set\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    # Training\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_acc = 0.0\n",
    "    for inputs, targets in train_loader:\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update statistics\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_acc += accuracy(outputs, targets) * inputs.size(0)\n",
    "\n",
    "    # Calculate average training loss and accuracy for the epoch\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    epoch_acc = running_acc / len(train_loader.dataset)\n",
    "\n",
    "    # Print training loss and accuracy for each epoch\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Training Loss: {epoch_loss:.4f}, Training Accuracy: {epoch_acc:.4f}\")\n",
    "\n",
    "    # Evaluate on the testing set\n",
    "    test_loss, test_accuracy = test_model(model, test_loader, criterion)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Testing Loss: {test_loss:.4f}, Testing Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), 'model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uzzeMzkldhcx"
   },
   "source": [
    "# Model Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "UlIGxQndpHxe"
   },
   "outputs": [],
   "source": [
    "def load_model(model_path, input_size, hidden_size, output_size):\n",
    "    model = NeuralNetwork(input_size, hidden_size, output_size)\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "# Function to preprocess the input sentence\n",
    "def preprocess_sentence(sentence, words):\n",
    "    sentence_words = sentence.lower().split()\n",
    "    sentence_words = [word for word in sentence_words if word in words]\n",
    "    return sentence_words\n",
    "\n",
    "# Function to convert the preprocessed sentence into a feature vector\n",
    "def sentence_to_features(sentence_words, words):\n",
    "    features = [1 if word in sentence_words else 0 for word in words]\n",
    "    return torch.tensor(features).float().unsqueeze(0)\n",
    "\n",
    "# Function to generate a response using the trained model\n",
    "def generate_response(sentence, model, words, classes):\n",
    "    sentence_words = preprocess_sentence(sentence, words)\n",
    "    if len(sentence_words) == 0:\n",
    "        return \"I'm sorry, but I don't understand. Can you please rephrase or provide more information?\"\n",
    "\n",
    "    features = sentence_to_features(sentence_words, words)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(features)\n",
    "\n",
    "    probabilities, predicted_class = torch.max(outputs, dim=1)\n",
    "    confidence = probabilities.item()\n",
    "    predicted_tag = classes[predicted_class.item()]\n",
    "\n",
    "    if confidence > 0.5:\n",
    "        for intent in intents['intents']:\n",
    "            if intent['tag'] == predicted_tag:\n",
    "                return random.choice(intent['responses'])\n",
    "\n",
    "    return \"I'm sorry, but I'm not sure how to respond to that.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9I5kQ1CHpPUM",
    "outputId": "65e27982-bf7d-4505-e96f-696e839a4a5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! I am a chatbot. How can I help you today? Type \"quit\" to exit.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">  hello\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello!\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">  quit\n"
     ]
    }
   ],
   "source": [
    "model_path = 'model.pth'\n",
    "input_size = len(words)\n",
    "hidden_size = 8\n",
    "output_size = len(classes)\n",
    "model = load_model(model_path, input_size, hidden_size, output_size)\n",
    "\n",
    "# Test the chatbot response\n",
    "print('Hello! I am a chatbot. How can I help you today? Type \"quit\" to exit.')\n",
    "while True:\n",
    "    user_input = input('> ')\n",
    "    if user_input.lower() == 'quit':\n",
    "        break\n",
    "    response = generate_response(user_input, model, words, classes)\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import scrolledtext, simpledialog, messagebox\n",
    "\n",
    "def send_message():\n",
    "    user_input = input_box.get(\"1.0\", tk.END).strip()\n",
    "    input_box.delete(\"1.0\", tk.END)\n",
    "    if user_input.lower() == 'quit':\n",
    "        root.destroy()\n",
    "    else:\n",
    "        response = generate_response(user_input, model, words, classes)\n",
    "        chat_history.config(state=tk.NORMAL)  # Allow modification to insert the response\n",
    "        chat_history.insert(tk.END, f\"User: {user_input}\\n\", \"user\")\n",
    "        chat_history.insert(tk.END, f\"Chatbot: {response}\\n\\n\", \"bot\")\n",
    "        chat_history.see(tk.END)\n",
    "        chat_history.config(state=tk.DISABLED)  # Disable modification after insertion\n",
    "        if response == \"I'm sorry, but I'm not sure how to respond to that.\" or response == \"I'm sorry, but I don't understand. Can you please rephrase or provide more information?\":\n",
    "            contribute_response_dialog(user_input)\n",
    "\n",
    "def contribute_response_dialog(user_input):\n",
    "    response = messagebox.askyesno(\"Contribute Response\", \"Contribute your response to train the bot for this question?\")\n",
    "    if response:\n",
    "        contribution_window = tk.Toplevel(root)\n",
    "        contribution_window.title(\"Contribute Response\")\n",
    "\n",
    "        # Labels and entry fields for user input\n",
    "        tk.Label(contribution_window, text=\"Roll Number:\").grid(row=0, column=0, padx=5, pady=5, sticky=\"e\")\n",
    "        tk.Label(contribution_window, text=\"Tag/Category:\").grid(row=1, column=0, padx=5, pady=5, sticky=\"e\")\n",
    "        tk.Label(contribution_window, text=\"Question:\").grid(row=2, column=0, padx=5, pady=5, sticky=\"e\")\n",
    "        tk.Label(contribution_window, text=\"Response:\").grid(row=3, column=0, padx=5, pady=5, sticky=\"e\")\n",
    "\n",
    "        roll_number_entry = tk.Entry(contribution_window)\n",
    "        roll_number_entry.grid(row=0, column=1, padx=5, pady=5, sticky=\"w\")\n",
    "        tag_category_entry = tk.Entry(contribution_window)\n",
    "        tag_category_entry.grid(row=1, column=1, padx=5, pady=5, sticky=\"w\")\n",
    "        question_entry = tk.Entry(contribution_window)\n",
    "        question_entry.grid(row=2, column=1, padx=5, pady=5, sticky=\"w\")\n",
    "        question_entry.insert(tk.END, user_input)  # Display the question in the entry field\n",
    "        response_entry = tk.Entry(contribution_window)\n",
    "        response_entry.grid(row=3, column=1, padx=5, pady=5, sticky=\"w\")\n",
    "\n",
    "        # Function to insert contribution into the database\n",
    "        def insert_contribution():\n",
    "            roll_number = roll_number_entry.get()\n",
    "            tag_category = tag_category_entry.get()\n",
    "            question = question_entry.get()\n",
    "            response = response_entry.get()\n",
    "\n",
    "            # Pattern generated based on user_input and bot's response\n",
    "            patterns = f\"User: {user_input}\"\n",
    "\n",
    "            try:\n",
    "                conn = sqlite3.connect(r\"C:/Users/HP/Downloads/chatbot_db.db\")\n",
    "                cursor = conn.cursor()\n",
    "                cursor.execute(\"INSERT INTO student_response (tag, patterns, responses, roll_number) VALUES (?, ?, ?, ?)\",\n",
    "                               (tag_category, patterns, response, roll_number))\n",
    "                conn.commit()\n",
    "                conn.close()\n",
    "                messagebox.showinfo(\"Contribution\", \"Thanks for your contribution! This will improve the bot's knowledge.\")\n",
    "                contribution_window.destroy()\n",
    "                continue_chatting()\n",
    "            except Exception as e:\n",
    "                messagebox.showerror(\"Error\", f\"An error occurred: {e}\")\n",
    "\n",
    "        # Button to submit the contribution\n",
    "        submit_button = tk.Button(contribution_window, text=\"Submit\", command=insert_contribution)\n",
    "        submit_button.grid(row=4, column=0, columnspan=2, padx=5, pady=10)\n",
    "    else:\n",
    "        continue_chatting()\n",
    "\n",
    "def continue_chatting():\n",
    "    chat_history.config(state=tk.DISABLED)  # Disable modification after insertion\n",
    "\n",
    "# Initialize the tkinter window\n",
    "root = tk.Tk()\n",
    "root.title(\"Chatbot\")\n",
    "\n",
    "# Set the window size and position\n",
    "window_width = 600  # Adjust as needed\n",
    "window_height = 600  # Adjust as needed\n",
    "screen_width = root.winfo_screenwidth()\n",
    "screen_height = root.winfo_screenheight()\n",
    "x_position = (screen_width - window_width) // 2\n",
    "y_position = (screen_height - window_height) // 2\n",
    "root.geometry(f\"{window_width}x{window_height}+{x_position}+{y_position}\")\n",
    "\n",
    "# Load and display the image\n",
    "image_path = \"C:/Users/HP/OneDrive/Pictures/Screenshots/Screenshot 2024-03-23 204802.png\"\n",
    "img = tk.PhotoImage(file=image_path)\n",
    "image_label = tk.Label(root, image=img)\n",
    "image_label.configure(width=400,height=200)\n",
    "image_label.pack()\n",
    "\n",
    "# Create a label for the text below the image\n",
    "text_label = tk.Label(root, text=\"L    B    R    C    E\", font=(\"Helvetica\", 12,\"bold\"))\n",
    "text_label.pack()\n",
    "\n",
    "# Create a frame for the chat interface with a white background\n",
    "chat_frame = tk.Frame(root, bg='brown')\n",
    "chat_frame.pack(fill=tk.BOTH, expand=True)\n",
    "\n",
    "# Create a scrolled text widget for chat history\n",
    "chat_history = scrolledtext.ScrolledText(chat_frame, width=50, height=15, bg='#25D366', fg='black', wrap=tk.WORD, font=(\"Helvetica\", 12))\n",
    "chat_history.pack(fill=tk.BOTH, expand=True)\n",
    "chat_history.tag_config(\"user\", foreground='black', background='yellow', justify=tk.RIGHT)\n",
    "chat_history.tag_config(\"bot\", foreground='white', background='#00703c', justify=tk.LEFT)\n",
    "chat_history.config(state=tk.DISABLED)  # Make chat history read-only\n",
    "\n",
    "# Display initial message from bot\n",
    "initial_message = \"Greetings! How can I help you today?\"\n",
    "chat_history.config(state=tk.NORMAL)  # Allow modification to insert the message\n",
    "chat_history.insert(tk.END, f\"Chatbot: {initial_message}\\n\\n\", \"bot\")\n",
    "chat_history.config(state=tk.DISABLED)  # Disable modification after insertion\n",
    "\n",
    "# Create a text box for user input\n",
    "input_box = tk.Text(chat_frame, width=40, height=3, font=(\"Helvetica\", 12))\n",
    "input_box.pack(side=tk.LEFT, padx=10, pady=10, fill=tk.X, expand=True)\n",
    "\n",
    "# Create a button to send user input with round shape\n",
    "send_button = tk.Button(chat_frame, text=\"Send\", width=10, command=send_message, bg='#9acd32', fg='white', font=(\"Helvetica\", 12, \"bold\"), relief=tk.RAISED, bd=3)\n",
    "send_button.pack(side=tk.LEFT, padx=10, pady=10)\n",
    "\n",
    "# Start the tkinter event loop\n",
    "root.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema for table 'student_response':\n",
      "tag VARCHAR(100) NULL\n",
      "patterns VARCHAR(100) NULL\n",
      "responses VARCHAR(100) NULL\n",
      "roll_number VARCHAR(100) NULL\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "\n",
    "def print_table_schema(database_path, table_name):\n",
    "    try:\n",
    "        conn = sqlite3.connect(database_path)\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(f\"PRAGMA table_info({table_name})\")\n",
    "        columns = cursor.fetchall()\n",
    "        \n",
    "        print(f\"Schema for table '{table_name}':\")\n",
    "        for column in columns:\n",
    "            column_name = column[1]\n",
    "            data_type = column[2]\n",
    "            nullable = \"NOT NULL\" if column[3] else \"NULL\"\n",
    "            print(f\"{column_name} {data_type} {nullable}\")\n",
    "    except sqlite3.Error as e:\n",
    "        print(\"Error:\", e)\n",
    "    finally:\n",
    "        if conn:\n",
    "            conn.close()\n",
    "\n",
    "# Example usage\n",
    "database_path = r\"C:/Users/HP/Downloads/chatbot_db.db\"\n",
    "table_name = \"student_response\"\n",
    "print_table_schema(database_path, table_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All records in table 'student_response':\n",
      "('greeting', 'Hi', 'Hello!', None)\n",
      "('greeting', 'Hello', 'Good to see you!', None)\n",
      "('farewell', 'Goodbye', 'Sad to see you go :(', None)\n",
      "('farewell', 'Bye', 'Goodbye!', None)\n",
      "('creator', 'Who created you?', 'I was created by Madhu Appala Narasimha Golthi.', None)\n",
      "('creator', 'Who made you?', 'I was created by Madhu Appala Narasimha Golthi.', None)\n",
      "('identity', 'What is your name?', \"You can call me LBRCE-BOT. I'm a Chatbot.\", None)\n",
      "('identity', 'Who are you?', \"You can call me LBRCE-BOT. I'm a Chatbot.\", None)\n",
      "('communication', 'How can I communicate with the college?', 'You can communicate with the college through our official email: lbcemym@lbrce.ac.in, helpline number: [7386349999, 9912030759],[08659 - 222933, 222934, 223936, 223937]', None)\n",
      "('communication', 'What are the communication channels for students and parents?', 'You can communicate with the college through our official email: lbcemym@lbrce.ac.in, helpline number: [7386349999, 9912030759],[08659 - 222933, 222934, 223936, 223937]', None)\n",
      "('canteen_items', \"User: quit\\nChatbot: ' Chatbot. I'm sorry, but I'm not sure how to respond to that.\", 'Egg puff', '21761A05F6')\n",
      "('Greeting of Hi', \"User: Hi\\nChatbot: ' Chatbot. I'm sorry, but I'm not sure how to respond to that.\", 'Hey Hi Welcome to LBRCE-BOT', '21761A05F7')\n",
      "('Greetings of Hi', 'User: Hi', 'Hey Hiyo Welcome to LBRCE-BOT', '21761A05F8')\n",
      "('About Canteen', 'User: Canteen', 'There are two Canteens', '21761A05F9')\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "\n",
    "def print_table_records(database_path, table_name):\n",
    "    try:\n",
    "        conn = sqlite3.connect(database_path)\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(f\"SELECT * FROM {table_name}\")\n",
    "        records = cursor.fetchall()\n",
    "        \n",
    "        print(f\"All records in table '{table_name}':\")\n",
    "        for record in records:\n",
    "            print(record)\n",
    "    except sqlite3.Error as e:\n",
    "        print(\"Error:\", e)\n",
    "    finally:\n",
    "        if conn:\n",
    "            conn.close()\n",
    "\n",
    "# Example usage\n",
    "database_path = r\"C:/Users/HP/Downloads/chatbot_db.db\"\n",
    "table_name = \"student_response\"\n",
    "print_table_records(database_path, table_name)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNpn+jahgm145n/Hl/QYowd",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
